{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q) retrieve the first 101 rows and the first 10 columns from the DataFrame\n",
    "df.iloc[0:101,0:10]\n",
    "# Q) extract the data from specific columns (2nd, 6th, 10th, 11th, 12th, and 17th) for all rows in the DataFrame\n",
    "df.iloc[:,[1,5,9,10,11,16]]\n",
    "# Q) select all rows of the DataFrame but only display columns starting from the second column(EmployeeName) to the end?\n",
    "df.iloc[:,1:]\n",
    "\n",
    "# select the first 11 rows of the DataFrame and all columns starting from 'Employee_Name' to the end?\n",
    "df1=df.loc[0:10,'EmployeeName':]\n",
    "# Q) acessing all rows and columns from the Employee_Name to Department\n",
    "df2=df.loc[:,'EmployeeName':'Department']\n",
    "# method 2\n",
    "df.loc[:,['EmployeeName','Gender','Age','BusinessTravel','Department']]\n",
    "\n",
    "# Q) extract the names, states, marital status, distance from home, departments, and job roles of employees who are older than 45 and work overtime?\n",
    "df3=df.loc[(df['Age']>45)&(df['OverTime']=='Yes'),['EmployeeName','State','MaritalStatus','DistanceFromHome (KM)','Department','JobRole']]\n",
    "\n",
    "# Q) select rows 100 to 500 and the columns from 'EmployeeName' to 'Education' in the DataFrame?\n",
    "df4=df.loc[100:500,'EmployeeName':'Education']\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'Salary' can be converted to numeric values\n",
    "df1['Salary'] = pd.to_numeric(df1['Salary'].str.replace('K', '').str.replace(',', ''), errors='coerce')\n",
    "\n",
    "# Interpolate missing values\n",
    "df1['Salary'] = df1['Salary'].interpolate()\n",
    "df1['Company Score'] = df1['Company Score'].fillna(mean_company_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance['ReviewDate'] = pd.to_datetime(df_performance['ReviewDate'], format='%m/%d/%Y')\n",
    "\n",
    "# let's convert 'ReviewDate' ino a date (as it can be seen above the format is M/D/Y)\n",
    "df_employee['HireDate'] = pd.to_datetime(df_employee['HireDate'], format='%Y-%m-%d')\n",
    "data['HireDate']=data['HireDate'].astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pandas.merge(performanceAnalysis, employeeData, on=\"EmployeeID\")\n",
    "# train_df.columns\n",
    "print(\"Data after merge: \")\n",
    "train_df.head().T\n",
    "\n",
    "print(train_df.isnull().any())\n",
    "\n",
    "print(\"\\nThere is \\033[1mNo Missing Values\\033[0m in the Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EducationLevel\n",
    "pd.merge(df_Employee,df_PerformanceRating,on='EmployeeID',how='inner')\n",
    "test_variable = pd.merge(df_EducationLevel,pd.merge(df_Employee,df_PerformanceRating,on='EmployeeID',how='inner'),left_on = 'EducationLevelID',right_on = 'Education')\n",
    "test_variable.drop(columns = ['EducationLevelID'],inplace=True )\n",
    "\n",
    "# All Satisfactions\n",
    "\n",
    "# EnvironmentSatisfaction\n",
    "test_variable = pd.merge(test_variable,df_SatisfiedLevel,left_on='EnvironmentSatisfaction',right_on = 'SatisfactionID',how='inner')\n",
    "test_variable.rename(columns={'SatisfactionLevel':'EnvironmentSatisfactionLevel'},inplace = True)\n",
    "test_variable.drop(columns=['SatisfactionID'],inplace = True)\n",
    "# JobSatisfaction\n",
    "test_variable = pd.merge(test_variable,df_SatisfiedLevel,left_on='JobSatisfaction',right_on = 'SatisfactionID',how='inner')\n",
    "test_variable.rename(columns={'SatisfactionLevel':'JobSatisfactionLevel'},inplace = True)\n",
    "test_variable.drop(columns=['SatisfactionID'],inplace = True)\n",
    "# RelationshipSatisfaction\n",
    "test_variable = pd.merge(test_variable,df_SatisfiedLevel,left_on='RelationshipSatisfaction',right_on = 'SatisfactionID',how='inner')\n",
    "test_variable.rename(columns={'SatisfactionLevel':'RelationshipSatisfactionLevel'},inplace = True)\n",
    "test_variable.drop(columns=['SatisfactionID'],inplace = True)\n",
    "\n",
    "# All Ratings\n",
    "\n",
    "# WorkLifeBalance\n",
    "test_variable = pd.merge(test_variable,df_RatingLevel,left_on='WorkLifeBalance',right_on = 'RatingID',how='inner')\n",
    "test_variable.rename(columns={'RatingLevel':'WorkLifeBalanceRatingLevel'},inplace = True)\n",
    "test_variable.drop(columns=['RatingID'],inplace = True)\n",
    "# Self Rating\n",
    "test_variable = pd.merge(test_variable,df_RatingLevel,left_on='SelfRating',right_on = 'RatingID',how='inner')\n",
    "test_variable.rename(columns={'RatingLevel':'SelfRatingLevel'},inplace = True)\n",
    "test_variable.drop(columns=['RatingID'],inplace = True)\n",
    "# Marager Rating\n",
    "test_variable = pd.merge(test_variable,df_RatingLevel,left_on='ManagerRating',right_on = 'RatingID',how='inner')\n",
    "test_variable.rename(columns={'RatingLevel':'ManagerRatingLevel'},inplace = True)\n",
    "test_variable.drop(columns=['RatingID'],inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "\n",
    "X = data.drop(['Person ID', 'Sleep Disorder'], axis=1)\n",
    "y = data['Sleep Disorder']\n",
    "\n",
    "# Calculation for mutual information\n",
    "mi_scores = mutual_info_classif(X, y)\n",
    "\n",
    "# dataframe of features and their mutual information scores\n",
    "mi_data = pd.DataFrame({'feature': X.columns, 'mi_score': mi_scores})\n",
    "mi_data = mi_data.sort_values('mi_score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(mi_data)\n",
    "\n",
    "# Visualize the top 15 features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(mi_data['feature'][:15], mi_data['mi_score'][:15])\n",
    "plt.title('Top 15 Features by Mutual Information')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Mutual Information Score')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for categorical features\n",
    "def plot_categorical_distribution(data, feature):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.countplot(x=feature, data=data) \n",
    "    plt.title(f\"Distribution of {feature}\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "    \n",
    "# Plot distributions for numerical features\n",
    "def plot_distributions(data, features):\n",
    "    for feature in features:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.histplot(data[feature], kde=True, bins=30, color='blue')\n",
    "        plt.title(f\"Distribution of {feature}\")\n",
    "        plt.xlabel(feature)\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the missing value summary\n",
    "def missing_value_summary(data):\n",
    "    # Calculate the number of missing values and their percentage\n",
    "    missing_value = data.isnull().sum().reset_index()\n",
    "    missing_value.columns = ['Variable', 'Number of Missing']\n",
    "    missing_value['Percentage of Missing'] = (missing_value['Number of Missing'] / data.shape[0] * 100).round(2)\n",
    "\n",
    "    # Sort the summary by the percentage of missing values in descending order\n",
    "    missing_value = missing_value.sort_values(by='Percentage of Missing', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return missing_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only numerical columns\n",
    "numerical_columns = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Calculating the Interquartile Range (IQR) for each numerical column\n",
    "Q1 = numerical_columns.quantile(0.25)\n",
    "Q3 = numerical_columns.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Defining the lower and upper bounds for outlier detection\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Finding outliers by checking if values fall outside the bounds\n",
    "outliers = (numerical_columns < lower_bound) | (numerical_columns > upper_bound)\n",
    "\n",
    "# Printing outliers for each numerical column\n",
    "for column in outliers.columns:\n",
    "    print(f\"Outliers in {column}:\")\n",
    "    print(df[outliers[column]][column])\n",
    "    print()\n",
    "    \n",
    "# Calculating the number of outliers in each numerical column\n",
    "num_outliers = outliers.sum()\n",
    "\n",
    "# Computing the percentage of outliers in each numerical column\n",
    "total_rows = df.shape[0]\n",
    "percentage_outliers = (num_outliers / total_rows) * 100\n",
    "\n",
    "# Creating a DataFrame to store the results\n",
    "outlier_stats = pd.DataFrame({\n",
    "    'Num_Outliers': num_outliers,\n",
    "    'Percentage_Outliers': percentage_outliers\n",
    "})\n",
    "\n",
    "# Displaying the outlier statistics\n",
    "print(\"Outlier Statistics:\")\n",
    "print(outlier_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize outliers\n",
    "def visualize_outliers(data, column, lower_bound, upper_bound):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Scatter plot to show outliers\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.scatterplot(x=data.index, y=data[column], color=\"blue\")\n",
    "    plt.axhline(lower_bound[column], color=\"red\", linestyle=\"--\", label=\"Lower Bound\")\n",
    "    plt.axhline(upper_bound[column], color=\"green\", linestyle=\"--\", label=\"Upper Bound\")\n",
    "    plt.title(f\"Scatter Plot of {column} (with Outliers)\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(column)\n",
    "    plt.legend()\n",
    "\n",
    "    # Boxplot to visualize outliers\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(x=data[column], color=\"lightblue\")\n",
    "    plt.title(f\"Boxplot of {column}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers for affected columns\n",
    "for col in [\"weight\", \"calories_burned\", \"bmi\"]:\n",
    "    visualize_outliers(df, col, lower_bound, upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis\n",
    "correlation_matrix = df.select_dtypes(include=[np.number]).corr()\n",
    "plt.figure(figsize=(16, 8))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, annot=True, mask=mask, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify key correlated pairs (threshold > 0.7 for strong correlation)\n",
    "correlation_threshold = 0.7\n",
    "key_correlated_pairs = []\n",
    "\n",
    "# Iterate over the correlation matrix and find pairs above the threshold\n",
    "for i in range(correlation_matrix.shape[0]):\n",
    "    for j in range(i + 1, correlation_matrix.shape[1]):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > correlation_threshold:\n",
    "            feature_1 = correlation_matrix.index[i]\n",
    "            feature_2 = correlation_matrix.columns[j]\n",
    "            correlation_value = correlation_matrix.iloc[i, j]\n",
    "            key_correlated_pairs.append((feature_1, feature_2, correlation_value))\n",
    "\n",
    "# Convert to DataFrame for better readability and display\n",
    "key_correlated_pairs_df = pd.DataFrame(key_correlated_pairs, columns=[\"Feature 1\", \"Feature 2\", \"Correlation\"])\n",
    "key_correlated_pairs_df = key_correlated_pairs_df.sort_values(by=\"Correlation\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "key_correlated_pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract correlation values between each feature and the target variable 'days_active_first_28_days_after_registration'\n",
    "target_correlation = correlation_matrix['calories_burned'].drop('calories_burned')\n",
    "\n",
    "# Sort correlations by absolute value to see the strongest correlations at the top\n",
    "target_correlation_sorted = target_correlation.sort_values(ascending=True).to_frame(name='Correlation with Target')\n",
    "\n",
    "# Plotting the correlations\n",
    "target_correlation_sorted.plot(kind='barh', figsize=(12,8), color='purple')\n",
    "plt.title(\"Correlations of Target Variable 'Calories_Burned' vs Numerical Attributes\")\n",
    "plt.xlabel(\"Correlation\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(['Attrition'])['Age'].mean()\n",
    "data.groupby(['Attrition'])['Department'].value_counts(normalize=True)\n",
    "data.groupby(['Attrition'])['JobRole'].value_counts(normalize=True)\n",
    "data.groupby(['Attrition'])['Salary'].value_counts(normalize=True)\n",
    "data.groupby(['Attrition'])['OverTime'].value_counts(normalize=True)\n",
    "\n",
    "data.groupby(['Department'])['Salary'].mean()\n",
    "data.groupby('JobRole')['Salary'].describe()\n",
    "data.groupby('Department')['OverTime'].value_counts(normalize=True)\n",
    "data.groupby('OverTime')['Attrition'].value_counts(normalize=True)\n",
    "data.groupby('Department')['YearsSinceLastPromotion'].mean()\n",
    "\n",
    "data.groupby('Attrition')['DistanceFromHome (KM)'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Department'].value_counts().plot(kind='bar', title='Employees per Department', color='green');\n",
    "data['Education'].value_counts().sort_index().plot(kind='bar', title='Education Level Distribution');\n",
    "data['Ethnicity'].value_counts().plot(kind='pie', autopct='%1.1f%%', title='Ethnicity Breakdown');\n",
    "data['Age'].plot(kind='hist', bins=20, title='Age Distribution', color='skyblue');\n",
    "data['BusinessTravel'].value_counts().plot(kind='bar', title='Business Travel Frequency', color='purple');\n",
    "data['YearsSinceLastPromotion'].plot(kind='hist', bins=10, title='Years Since Last Promotion', color='teal');\n",
    "data['Salary'].plot(kind='hist', bins=20, title='Salary Distribution', color='navy');\n",
    "data.groupby('JobRole')['Salary'].mean().sort_values().plot(kind='barh', title='Average Salary by Job Role', figsize=(10,8), color='olive');\n",
    "data['Attrition'].value_counts().plot(kind='pie', autopct='%1.1f%%', title='Overall Attrition Rate');\n",
    "\n",
    "plt.scatter(data['YearsAtCompany'], data['Salary'], alpha=0.5, color='magenta')\n",
    "plt.title('Salary vs. Years at Company')\n",
    "plt.xlabel('Years at Company')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()\n",
    "\n",
    "attrition_by_dept = data.groupby('Department')['Attrition'].value_counts(normalize=True).unstack() * 100\n",
    "attrition_by_dept.plot(kind='bar', stacked=True, title='Attrition Rate by Department', color=['lightgreen', 'salmon']);\n",
    "\n",
    "sns.countplot(x='OverTime', hue='Attrition', data=data)\n",
    "plt.title('Attrition in Relation to Overtime');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='DistanceFromHome (KM)', hue='Attrition', multiple='stack')\n",
    "plt.title('Distance from Home vs. Attrition')\n",
    "plt.xlabel('Distance from Home (KM)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=df, x='YearsAtCompany', y='Salary', hue='Attrition')\n",
    "plt.title('Salary vs. Years at Company by Attrition')\n",
    "plt.xlabel('Years at Company')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(data_frame=pd.DataFrame(df['Age'].value_counts()).sort_values(by = 'Age').reset_index(),x = 'Age',y = 'count',title='Age Distribution')\n",
    "# It seems to follow the distribution, however i don't like the records where age is greater or equal to 49 age, there is a rapid decline in the distribution.\n",
    "# This considered to be outlier and must be dropped from the data\n",
    "index = df[df['Age'] >= 49].index.tolist()\n",
    "df.drop(labels=index,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(data_frame=df['Salary'].value_counts().reset_index(),y = 'Salary',x = 'count',color = 'Salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df['JobRole'].value_counts()).reset_index().sort_values(by = 'JobRole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "#Age by Gender\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.boxplot(x='Gender', y='Age', data=df)\n",
    "plt.title('Age by Gender')\n",
    "\n",
    "#Salary by Department\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.boxplot(x='Department', y='Salary', data=df)\n",
    "plt.title('Salary by Department')\n",
    "\n",
    "# DistanceFromHome by BusinessTravel\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.boxplot(x='BusinessTravel', y='DistanceFromHome (KM)', data=df)\n",
    "plt.title('DistanceFromHome by BusinessTravel')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attrition Percentage\n",
    "Attrition_counts = df['Attrition'].value_counts()\n",
    "plt.pie(x=Attrition_counts.values, labels=Attrition_counts.index, autopct='%1.1f%%', startangle=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Age distribution by Attrition\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Attrition', y='Age', data=df, palette='pastel')\n",
    "plt.title('Age Distribution by Attrition')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attrition by JobRole\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='JobRole', hue='Attrition', data=df, palette='Set1')\n",
    "plt.title('Attrition by JobRole')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate proportions\n",
    "crosstab = pd.crosstab(employeeData['JobRole'], employeeData['Attrition'])\n",
    "proportions = crosstab.div(crosstab.sum(axis=1), axis=0)\n",
    "proportions = proportions.sort_values(by='Yes', ascending=False)\n",
    "plt.figure(figsize=(10, 6))\n",
    "proportions.plot(kind='bar', stacked=True)\n",
    "\n",
    "# Adjust legend\n",
    "plt.title('Attrition by Job Role')\n",
    "plt.xlabel('Job Role')\n",
    "plt.ylabel('Proportion')\n",
    "plt.xticks(rotation=85)\n",
    "plt.legend(title='Attrition', loc='upper left', bbox_to_anchor=(1, 1), fontsize='small')\n",
    "plt.tight_layout()  # Adjust layout to make room for the legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "df['AttritionNumeric'] = df['Attrition'].replace({'Yes': 1, 'No': 0})\n",
    "columns_for_correlation = ['AttritionNumeric'] + numerical_cols  # Include any numerical columns\n",
    "correlation_matrix = df[columns_for_correlation].corr()\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relation between Education and Job Role\n",
    "\n",
    "countEduJob = pd.crosstab(employeeData['EducationField'], employeeData['JobRole'])\n",
    "\n",
    "total_counts = countEduJob.sum(axis=1)\n",
    "sorted_education_indices = total_counts.sort_values(ascending=False).index\n",
    "sorted_job_roles = countEduJob.loc[sorted_education_indices].sum(axis=0).sort_values(ascending=False).index\n",
    "sorted_contingency_table = countEduJob.loc[sorted_education_indices, sorted_job_roles]\n",
    "\n",
    "ax = sorted_contingency_table.plot(kind='bar', stacked=True)\n",
    "\n",
    "# Adjust layout\n",
    "ax.legend(title='Job Role', loc='upper left', bbox_to_anchor=(1, 1), fontsize='small')\n",
    "plt.title('Relationship between Education Field and Job Role')\n",
    "plt.xlabel('Education Field')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=85)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(6,1, figsize=(18,30))\n",
    "plt.suptitle('Employee characteristics versus Gender and Ethnicity', ha='center', fontsize=18, fontweight='bold',color=\"k\",y=0.92)\n",
    "a = sns.boxplot(data=df_employee, x='Gender',hue='Ethnicity', y='Salary',ax=axs[0])\n",
    "axs[0].legend(loc=\"upper right\", bbox_to_anchor=(1.12,1))\n",
    "axs[0].set_ylabel('Salary',fontweight='bold')\n",
    "axs[0].set_xlabel('')\n",
    "sns.boxplot(data=df_employee, x='Gender',hue='Ethnicity', y='YearsAtCompany',ax=axs[1])\n",
    "axs[1].legend(loc=\"upper right\", bbox_to_anchor=(1.12,1))\n",
    "axs[1].set_ylabel('Years At Company',fontweight='bold')\n",
    "axs[1].set_xlabel('')\n",
    "sns.boxplot(data=df_employee, x='Gender',hue='Ethnicity', y='YearsInMostRecentRole',ax=axs[2])\n",
    "axs[2].legend(loc=\"upper right\", bbox_to_anchor=(1.12,1))\n",
    "axs[2].set_ylabel('Years In Most Recent Role',fontweight='bold')\n",
    "axs[2].set_xlabel('')\n",
    "sns.boxplot(data=df_employee, x='Gender',hue='Ethnicity', y='YearsSinceLastPromotion',ax=axs[3])\n",
    "axs[3].legend(loc=\"upper right\", bbox_to_anchor=(1.12,1))\n",
    "axs[3].set_ylabel('Years Since Last Promotion',fontweight='bold')\n",
    "axs[3].set_xlabel('')\n",
    "sns.boxplot(data=df_employee, x='Gender',hue='Ethnicity', y='YearsWithCurrManager',ax=axs[4])\n",
    "axs[4].legend(loc='upper right', bbox_to_anchor=(1.12,1))\n",
    "axs[4].set_ylabel('Years With Curr Manager',fontweight='bold')\n",
    "axs[4].set_xlabel('')\n",
    "sns.boxplot(data=df_employee, x='Gender',hue='Ethnicity', y='DistanceFromHome (KM)',ax=axs[5])\n",
    "axs[5].legend(loc='upper right', bbox_to_anchor=(1.12,1))\n",
    "axs[5].set_ylabel('Distance From Home (KM)',fontweight='bold')\n",
    "axs[5].set_xlabel('Gender',fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Plot data on each subplot\n",
    "plt.suptitle(' Employee characteristics evolution', ha='center', fontsize=18, fontweight='bold',color='darkblue',y=0.96)\n",
    "df_gender_evolution.plot(ax=axs[0, 0], title='Gender Evolution')\n",
    "df_ethnicity_evolution.plot(ax=axs[0, 1], title='Ethnicity Evolution')\n",
    "df_education_evolution.plot(ax=axs[1, 0], title='Eduction Evolution')\n",
    "df_attrition_evolution.plot(ax=axs[1, 1], title='Attrition Evolution')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/pierrejeanne/employee-attrition-performance-eda-visualization\n",
    "\n",
    "CONCLUSION\n",
    "\n",
    "we did not observed any inequality between man and woman in therms of role, salary or background.\n",
    "they are more white people (almost half of the company),but it is less that the percentage of white people living in the US (61.6%).\n",
    "White people have also the highest salaries, but it seems the company was created by them.\n",
    "Most of the employee live in California, where the 40% of the population is Latinos. This ethnic group is not represented, in California latinos are mostly gardener, farm worker or construction worker..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.kaggle.com/code/pierrejeanne/employee-attrition-performance-eda-visualization\n",
    "- https://www.kaggle.com/code/jimysalem/data-visualization-hr {not summarised here, but good distribution plots}\n",
    "- https://www.kaggle.com/code/adarshvishwakarma12/hr-analytics-reducing-employee-turnover { pipelines, neural network, etc. }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Year and Month of review\n",
    "df_employee['HireDate'] = df_employee['HireDate'].dt.to_period('M')\n",
    "# Count the Number of Dates per Month\n",
    "nb_employee_per_month = df_employee.groupby('HireDate').size()\n",
    "\n",
    "### calculate nb of review per months\n",
    "# Extract Year and Month of review\n",
    "df_performance['review_Y_M'] = df_performance['ReviewDate'].dt.to_period('M')\n",
    "# Count the Number of Dates per Month\n",
    "nb_review_per_month = df_performance.groupby('review_Y_M').size()\n",
    "\n",
    "## merge two series\n",
    "df_count_review   = pd.DataFrame({'Month':nb_review_per_month.index, 'count_review':nb_review_per_month.values})\n",
    "df_count_employee = pd.DataFrame({'Month':nb_employee_per_month.index, 'count_employee':nb_employee_per_month.values})\n",
    "df_count_review   = df_count_review.set_index('Month')\n",
    "df_count_employee = df_count_employee.set_index('Month')\n",
    "df_count_employee['cum_nb_employee'] = df_count_employee['count_employee'].cumsum()\n",
    "df_count_employee.head(2)\n",
    "\n",
    "# Step 1: Combine the indexes of both DataFrames\n",
    "all_dates = df_count_employee.index.union(df_count_review.index)\n",
    "\n",
    "# Step 2: Reindex both DataFrames to include all dates\n",
    "df_count_review = df_count_review.reindex(all_dates)\n",
    "df_count_employee = df_count_employee.reindex(all_dates)\n",
    "\n",
    "# Step 3: Merge the DataFrames\n",
    "df_count = pd.concat([df_count_review, df_count_employee], axis=1).fillna(0)\n",
    "\n",
    "# Ensure 'Month' is a string (for x-axis)\n",
    "df_count = df_count.reset_index()\n",
    "df_count['Month'] = df_count['Month'].dt.strftime('%Y-%m')\n",
    "\n",
    "df_count.head(2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 4))\n",
    "\n",
    "# Plot bar plot with Seaborn\n",
    "sns.barplot(x=df_count.index, y='count_review', data=df_count, color='blue', ax=ax, label='Monthly Reviews')\n",
    "\n",
    "# Plot line plot with Seaborn\n",
    "sns.lineplot(x=df_count.index, y='count_employee', data=df_count, marker='o', color='red', ax=ax, label='monthly hired')\n",
    "ax2 = ax.twinx()\n",
    "sns.lineplot(x=df_count.index, y='cum_nb_employee', data=df_count, marker='o', color='green', ax=ax2, label='Number of Employees')\n",
    "\n",
    "\n",
    "# Adding title and labels\n",
    "ax.set_title('Number of Reviews and Employees per Month',color='darkblue')\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "# Customize x-axis to show only every 3 months\n",
    "xticks_labels = df_count['Month'].unique()\n",
    "every_n_months = 3\n",
    "ax.set_xticks(range(0, len(xticks_labels), every_n_months))\n",
    "ax.set_xticklabels(xticks_labels[::every_n_months], rotation=45, ha='right')\n",
    "\n",
    "# Add legends\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(0, 0.8))\n",
    "ax2.legend(loc='upper left', bbox_to_anchor=(0, 0.8))\n",
    "\n",
    "# Display the plot\n",
    "ax2.tick_params(axis='y', labelcolor='g')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_encoded[selected_features]\n",
    "y = df_encoded['calories_burned']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=55)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, stratify=y_train, random_state=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform train, validation, and test sets\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Check the shapes of the scaled data\n",
    "print(\"Scaled Train set size:\", X_train_scaled.shape)\n",
    "print(\"Scaled Validation set size:\", X_val_scaled.shape)\n",
    "print(\"Scaled Test set size:\", X_test_scaled.shape)\n",
    "\n",
    "# Save the scaler\n",
    "scaler_path = os.path.join(processed_data_folder, 'scaler.pkl')\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Scaler saved at: {scaler_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "# Data Manipulation and Processing\n",
    "import joblib\n",
    "\n",
    "# Data Visualization\n",
    "import shap\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, LeaveOneOut\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Statistical Analysis\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error\n",
    "\n",
    "# Other Libraries\n",
    "import optuna\n",
    "import opendatasets as od\n",
    "\n",
    "# Set Plotting Defaults\n",
    "sns.set(rc={'figure.figsize': (16, 8)})\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LOOCV\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(),\n",
    "    \"Lasso Regression\": Lasso(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    \"Support Vector Regressor\": SVR(),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=100, random_state=42),\n",
    "    \"LightGBM\": LGBMRegressor(random_state=42, n_estimators=100, force_col_wise=True),\n",
    "    \"CatBoost\": CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, verbose=0, random_state=42)\n",
    "}\n",
    "\n",
    "# Evaluate each model using LOOCV\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "    for train_index, test_index in loo.split(X_train_scaled):\n",
    "        X_train_split, X_test_split = X_train_scaled[train_index], X_train_scaled[test_index]\n",
    "        y_train_split, y_test_split = y_train.values[train_index], y_train.values[test_index]\n",
    "        \n",
    "        model.fit(X_train_split, y_train_split)\n",
    "        pred = model.predict(X_test_split)\n",
    "        \n",
    "        predictions.append(pred[0])\n",
    "        true_values.append(y_test_split[0])\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predictions))\n",
    "    results[name] = rmse\n",
    "\n",
    "# Display LOOCV results\n",
    "results = dict(sorted(results.items(), key=lambda x: x[1]))\n",
    "print(\"Model Performance (LOOCV - RMSE):\")\n",
    "for model_name, rmse in results.items():\n",
    "    print(f\"{model_name}: {rmse:.2f}\")\n",
    "    \n",
    "pd.DataFrame(results.items(), columns=([\"Model\",\"RMSE\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model based on the lowest RMSE\n",
    "best_model_name = min(results, key=results.get)\n",
    "best_model_rmse = results[best_model_name]\n",
    "\n",
    "print(\"\\nBest Model Based on LOOCV:\")\n",
    "print(f\"Model: {best_model_name}\")\n",
    "print(f\"RMSE: {best_model_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 500, 2000),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-3, 10.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"random_state\": 42,\n",
    "        \"verbose\": 0\n",
    "    }\n",
    "\n",
    "    # Train CatBoost\n",
    "    model = CatBoostRegressor(**params)\n",
    "    model.fit(X_train_scaled, y_train, eval_set=(X_val_scaled, y_val), early_stopping_rounds=50)\n",
    "\n",
    "    # Predict and calculate RMSE\n",
    "    y_pred = model.predict(X_val_scaled)\n",
    "    rmse = root_mean_squared_error(y_val, y_pred)\n",
    "    return rmse\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best RMSE:\", study.best_value)\n",
    "\n",
    "best_hyperparams = study.best_params\n",
    "\n",
    "# Train the final model\n",
    "final_model = CatBoostRegressor(\n",
    "    iterations=best_hyperparams['iterations'],\n",
    "    learning_rate=best_hyperparams['learning_rate'],\n",
    "    depth=best_hyperparams['depth'],\n",
    "    l2_leaf_reg=best_hyperparams['l2_leaf_reg'],\n",
    "    subsample=best_hyperparams['subsample'],\n",
    "    verbose=0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "final_model.fit(X_final_train, y_final_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "test_predictions = final_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate performance on the test set\n",
    "test_rmse = root_mean_squared_error(y_test, test_predictions)\n",
    "print(f\"Final Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "# Plot predicted vs actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, test_predictions, alpha=0.6, color='blue')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs Predicted Values (Test Set)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP values\n",
    "shap_values = final_model.get_feature_importance(\n",
    "    Pool(X_train_scaled, y_train),\n",
    "    type=\"ShapValues\"\n",
    ")\n",
    "\n",
    "# Exclude the base value column (last column in shap_values)\n",
    "shap_values_for_features = shap_values[:, :-1]\n",
    "\n",
    "# Create a SHAP summary plot\n",
    "shap.summary_plot(shap_values_for_features, X_train, feature_names=selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 2, 50, 100],\n",
    "    'max_iter': [500, 1000]\n",
    "}\n",
    "\n",
    "# The logistic regression model\n",
    "lr = LogisticRegression(random_state=55)\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(\n",
    "    lr,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='balanced_accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best estimator\n",
    "lr_best = grid_search.best_estimator_\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation balanced accuracy score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluation on validation set\n",
    "val_pred = lr_best.predict(X_val)\n",
    "val_bal = balanced_accuracy_score(y_val, val_pred)\n",
    "val_precision = precision_score(y_val, val_pred, average='weighted')\n",
    "val_recall = recall_score(y_val, val_pred, average='weighted')\n",
    "val_f1 = f1_score(y_val, val_pred, average='weighted')\n",
    "\n",
    "\n",
    "print(\"Validation balanced accuracy:\", val_bal)\n",
    "print(\"Validation Precision:\", val_precision)\n",
    "print(\"Validation Recall:\", val_recall)\n",
    "print(\"Validation F1 Score:\", val_f1)\n",
    "\n",
    "# Evaluation on test set\n",
    "test_pred = lr_best.predict(X_test)\n",
    "test_bal = balanced_accuracy_score(y_test, test_pred)\n",
    "test_precision = precision_score(y_test, test_pred, average='weighted')\n",
    "test_recall = recall_score(y_test, test_pred, average='weighted')\n",
    "test_f1 = f1_score(y_test, test_pred, average='weighted')\n",
    "\n",
    "print(\"Test balanced accuracy:\", test_bal)\n",
    "print(\"Test Precision:\", test_precision)\n",
    "print(\"Test Recall:\", test_recall)\n",
    "print(\"Test F1 Score:\", test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting model\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=55)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = dt.predict(X_val)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "round(acc, 3)\n",
    "\n",
    "from sklearn.tree import export_text\n",
    "print(export_text(dt, feature_names=list(dv.get_feature_names_out())))\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(dt, feature_names=dv.feature_names_, class_names=['Edible', 'Poisonous'], \n",
    "          filled=True, rounded=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target and features\n",
    "X = df_encoded.drop(columns=['calories_burned'], errors='ignore')\n",
    "y = df_encoded['calories_burned']\n",
    "\n",
    "# Split the data with shuffling enabled for feature selection\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Initialize the Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model using only the training set\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance from the Random Forest model\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Initialize RFE with the Random Forest model as the estimator\n",
    "rfe_selector = RFE(estimator=rf_model, n_features_to_select=10, step=1)\n",
    "rfe_selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the ranking of features from RFE\n",
    "rfe_features = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'RFE_Ranking': rfe_selector.ranking_\n",
    "}).sort_values(by='RFE_Ranking')\n",
    "\n",
    "# Merge feature importance and RFE rankings for comparison\n",
    "feature_selection_summary = pd.merge(feature_importances, rfe_features, on='Feature')\n",
    "\n",
    "# Select features with RFE rank 1\n",
    "selected_features = feature_selection_summary[\n",
    "    feature_selection_summary['RFE_Ranking'] == 1\n",
    "]['Feature'].tolist()\n",
    "\n",
    "print(\"Selected Features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for s in df_scores['min_samples_leaf'].unique():\n",
    "    subset = df_scores[df_scores['min_samples_leaf'] == s]\n",
    "    plt.plot(subset['n_estimators'], subset['auc'], \n",
    "             label=f'min_samples_leaf={s}', marker='o')\n",
    "\n",
    "plt.title('AUC Scores by Parameters')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for s in df_scores['min_samples_leaf'].unique():\n",
    "    subset = df_scores[df_scores['min_samples_leaf'] == s]\n",
    "    plt.plot(subset['n_estimators'], subset['acc'],\n",
    "             label=f'min_samples_leaf={s}', marker='o')\n",
    "\n",
    "plt.title('Accuracy Scores by Parameters')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Plot AUC scores\n",
    "plt.subplot(1, 2, 1)\n",
    "for d in [5, 8, 10]:\n",
    "    df_subset = df_scores[df_scores.max_depth == d]\n",
    "    plt.plot(df_subset.n_estimators, df_subset.auc,\n",
    "             label=f'max_depth={d}', marker='o')\n",
    "\n",
    "plt.title('AUC Scores by Parameters')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Accuracy scores\n",
    "plt.subplot(1, 2, 2)\n",
    "for d in [5, 8, 10]:\n",
    "    df_subset = df_scores[df_scores.max_depth == d]\n",
    "    plt.plot(df_subset.n_estimators, df_subset.acc,\n",
    "             label=f'max_depth={d}', marker='o')\n",
    "\n",
    "plt.title('Accuracy Scores by Parameters')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical values\n",
    "\n",
    "# Value of 'BusinessTrip' is ordinal type and will be encoded using OrdinalEncoder, while other column could be classified as nominal type and we can use OneHotEncoder for encoding.\n",
    "\n",
    "#categorical value encoding\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def cat_encoder(data):\n",
    "    \n",
    "    ordinal_cols = ['BusinessTravel']\n",
    "    nominal_cols = ['Gender', 'Department', 'MaritalStatus']\n",
    "    \n",
    "    #handling missing values (in case of any)\n",
    "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    data.loc[:, ordinal_cols + nominal_cols] = cat_imputer.fit_transform(data[ordinal_cols + nominal_cols])\n",
    "    \n",
    "    transformers=[\n",
    "          ('ordinal', OrdinalEncoder(), ordinal_cols),\n",
    "          ('nominal', OneHotEncoder(sparse_output=False), nominal_cols)\n",
    "      ]\n",
    "    encoder = ColumnTransformer(transformers=transformers, remainder='passthrough')\n",
    "    encoded_data = encoder.fit_transform(data)\n",
    "    encoded_data = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(input_features=data.columns), index=data.index)\n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=4, figsize=(12, 8))\n",
    "axes = ax.flatten()\n",
    "\n",
    "for i, col in enumerate(X_train_num):\n",
    "    sns.kdeplot(X_train[col], ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution of {col}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#numerical value imputation\n",
    "\n",
    "def num_imputer(data):\n",
    "    num_imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    imputed_data = num_imputer.fit_transform(data)\n",
    "    imputed_data = pd.DataFrame(imputed_data, columns=data.columns, index=data.index)\n",
    "    return imputed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat_encoded = cat_encoder(X_train_cat)\n",
    "X_train_cat_encoded.head()\n",
    "\n",
    "X_train_num_imputed = num_imputer(X_train_num)\n",
    "X_train_num_imputed.isnull().sum()\n",
    "\n",
    "#re-combine the categorical & numerical data\n",
    "\n",
    "X_train_imputed = pd.concat([X_train_cat_encoded, X_train_num_imputed], axis=1)\n",
    "X_train_imputed.shape\n",
    "\n",
    "X_train_clean = standardize_data(X_train_imputed)\n",
    "X_train_clean.head()\n",
    "\n",
    "def preprocess_data(data, num_cols, cat_cols, cat_encoder, num_imputer, standardize_data):\n",
    "    X_num, X_cat = split_catnum(data, num_cols, cat_cols)\n",
    "    X_cat_encoded = cat_encoder(X_cat)\n",
    "    X_num_imputed = num_imputer(X_num)\n",
    "    X_imputed = pd.concat([X_cat_encoded, X_num_imputed], axis=1)\n",
    "    X_clean = standardize_data(X_imputed)\n",
    "    return X_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "for col in categorical_cols:\n",
    "    contingency_table = pd.crosstab(data[col], data['Attrition'])\n",
    "    chi2, p, dof, ex = chi2_contingency(contingency_table)\n",
    "    print(f\"Chi-Square test between 'Attrition' and '{col}': p-value = {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- P-Value < 0.05: Indicates a statistically significant association between 'Attrition' and the feature. This means the feature is likely relevant for predicting 'Attrition'.\n",
    "\n",
    "- P-Value  0.05: Indicates that the association is not statistically significant. The feature may not be as relevant for predicting 'Attrition'.\n",
    "\n",
    "- Features like BusinessTravel, JobRole, OverTime, and MaritalStatus have very low p-values and should be considered as strong predictors for 'Attrition'.\n",
    "\n",
    "- Features like Department, EducationField, though significant, might have weaker associations. Depending on model performance, may decide to keep or exclude them.\n",
    "\n",
    "- Features with high p-values (e.g., Gender, State, Ethnicity) might not contribute much to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = data.select_dtypes(include=['number'])\n",
    "correlation_matrix = numeric_data.corr()\n",
    "correlation_with_target = correlation_matrix['Attrition'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature YearsSinceLastPromotion has the strongest negative correlation with 'Attrition', indicating that employees who have not been promoted recently are more likely to leave.\n",
    "\n",
    "Features like DistanceFromHome and Education have very weak correlations with 'Attrition', suggesting they have minimal impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(x_train, y_train_smote)\n",
    "y_pred=model.predict(x_test)\n",
    "print(classification_report(y_test_smote, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test_smote, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'Decision Tree': DecisionTreeClassifier()\n",
    "}\n",
    "def evaluate_models(models, x_train, y_train,x_test,y_test):\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        print(f\"Evaluating {name}\")\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "\n",
    "        scores = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "            'recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "            'f1': f1_score(y_test, y_pred, average='weighted')\n",
    "        }\n",
    "\n",
    "        print(f\"{name} - Accuracy: {scores['accuracy']:.4f}, Precision: {scores['precision']:.4f}, \"\n",
    "              f\"Recall: {scores['recall']:.4f}, F1 Score: {scores['f1']:.4f}\")\n",
    "\n",
    "        results[name] = scores\n",
    "        print('=====================================================================================')\n",
    "\n",
    "    return results\n",
    "\n",
    "evaluate_models(models, x_train, y_train_smote,x_test,y_test_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 250,300],  # Number of trees\n",
    "    'max_depth': [50,70,80,90],  # Tree depth\n",
    "    'min_samples_split': [ 5, 6,7],  # Minimum samples to split a node\n",
    "    'min_samples_leaf': [1, 2],    # Minimum samples in a leaf\n",
    "    'max_features': ['sqrt'], # Number of features to consider for best split\n",
    "    'bootstrap': [True],       # Whether bootstrap samples are used\n",
    "    'criterion': ['gini']  # Split criterion\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=RF, param_grid=param_grid,\n",
    "                           cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "\n",
    "# Fit the model on training data\n",
    "grid_search.fit(x_train, y_train_smote)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)\n",
    "\n",
    "# Best model evaluation\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "y_pred = best_rf_model.predict(x_test)\n",
    "accuracy_score(y_test_smote, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    " \n",
    "training_columns=['Age', 'Salary', 'YearsAtCompany', 'YearsInMostRecentRole',\n",
    "       'YearsSinceLastPromotion', 'BusinessTravel_No Travel ',\n",
    "       'BusinessTravel_Some Travel', 'Department_Sales',\n",
    "       'Department_Technology', 'JobRole_Data Scientist',\n",
    "       'JobRole_Engineering Manager', 'JobRole_HR Business Partner',\n",
    "       'JobRole_HR Executive', 'JobRole_HR Manager',\n",
    "       'JobRole_Machine Learning Engineer', 'JobRole_Manager',\n",
    "       'JobRole_Recruiter', 'JobRole_Sales Executive',\n",
    "       'JobRole_Sales Representative', 'JobRole_Senior Software Engineer',\n",
    "       'JobRole_Software Engineer', 'MaritalStatus_Married',\n",
    "       'MaritalStatus_Single', 'OverTime_Yes']\n",
    "\n",
    "with open('columns.pkl','wb')as file:\n",
    "    pickle.dump(training_columns,file)\n",
    "    \n",
    "with open('model_1.pkl', 'wb') as file:\n",
    "    pickle.dump(best_rf_model, file)\n",
    "\n",
    "with open('scaler_1.pkl','wb')as file:\n",
    "    pickle.dump(scaler,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Define the parameter grid\n",
    "param_dist = {\n",
    "    'n_estimators': [int(x) for x in np.linspace(start=100, stop=1000, num=10)],\n",
    "    'max_depth': [int(x) for x in np.linspace(10, 110, num=11)],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "# Create the RandomizedSearchCV object\n",
    "RF=RandomForestClassifier()\n",
    "random_search = RandomizedSearchCV(estimator=RF, param_distributions=param_dist, n_iter=100, cv=5, scoring='accuracy', verbose=2, n_jobs=-1, random_state=42)\n",
    "\n",
    "# # Fit the model to the data\n",
    "random_search.fit(x_train, y_train_smote)\n",
    "\n",
    "print(\"Best Parameters:\",  random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)\n",
    "\n",
    "# Best model evaluation\n",
    "best_rf = random_search.best_estimator_\n",
    "y_pred = best_rf.predict(x_test)\n",
    "accuracy_score(y_test_smote, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBOOST CLASSIFIER\n",
    "\n",
    "# defining data (X and y)\n",
    "X = train_df_transformed.drop('Attrition', axis=1)\n",
    "y = train_df_transformed['Attrition']\n",
    "\n",
    "# Calculate scale_pos_weight (since the target variable is imbalanced!)\n",
    "neg_class = len(y[y == 0])  # Count of class 0 (non-attrition)\n",
    "pos_class = len(y[y == 1])  # Count of class 1 (attrition)\n",
    "scale_pos_weight = neg_class / pos_class\n",
    "\n",
    "# fitting the XGBClassifier Model\n",
    "model = xgb.XGBClassifier(scale_pos_weight=scale_pos_weight)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "features = X.columns\n",
    "feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "top_features_df = feature_importance_df.head(7) # filtering the top 7 features!\n",
    "\n",
    "# Plot the feature importance\n",
    "# plt.style.use('dark_background')\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_features_df['Feature'], top_features_df['Importance'], color='skyblue')\n",
    "\n",
    "plt.xlabel('Feature Importance', fontsize=13)\n",
    "plt.ylabel('Features', fontsize=13)\n",
    "plt.title('Top 7 Most Important Features using XGBoost')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to show the most important feature on top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"# AUC: {roc_auc_score(y_va, model1.predict_proba(X_va)[:,1]):.5f}\")\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_curve, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Predict on validation data\n",
    "y_pred = model.predict(X_va)\n",
    "y_pred_proba = model.predict_proba(X_va)[:, 1]  # For ROC-AUC curve (binary classification)\n",
    "\n",
    "# 1. Classification report (Precision, Recall, F1-Score, and Accuracy)\n",
    "print(classification_report(y_va, y_pred))\n",
    "\n",
    "# 2. Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_va, y_pred)\n",
    "ConfusionMatrixDisplay(confusion_matrix=conf_matrix).plot(cmap='Blues')\n",
    "\n",
    "# 3. ROC-AUC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_va, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_va, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, recall_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# * Create Baseline model (DummyClassifier)\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy='stratified', random_state=123)\n",
    "dummy_clf.fit(X_train_clean, y_train)\n",
    "y_pred_dummy = dummy_clf.predict(X_valid_clean)\n",
    "\n",
    "dummy_score = recall_score(y_valid, y_pred_dummy, average='weighted')\n",
    "print(dummy_score)\n",
    "\n",
    "# * Model Selection (with hyperparameter tuning using GridSearchCV)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Logistic Regression\n",
    "\n",
    "logreg_cv = GridSearchCV(estimator=LogisticRegression(solver='liblinear'),\n",
    "                          param_grid={'penalty': ['l1', 'l2'],\n",
    "                                      'C': np.logspace(-4,4,20)},\n",
    "                          cv=5,\n",
    "                          scoring='recall_weighted',\n",
    "                          verbose=1,\n",
    "                          return_train_score=True)\n",
    "\n",
    "logreg_cv.fit(X_train_clean, y_train)\n",
    "print(f\"Best parameter: {logreg_cv.best_params_}\")\n",
    "\n",
    "#train model using best parameter\n",
    "lr_clf = LogisticRegression(solver='liblinear', C=0.08858667904100823, penalty='l2')\n",
    "lr_clf.fit(X_train_clean, y_train)\n",
    "\n",
    "y_pred_lr = lr_clf.predict(X_valid_clean)\n",
    "\n",
    "lr_score = recall_score(y_valid, y_pred_lr, average='weighted')\n",
    "print(lr_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DecisionTree Classifier\n",
    "\n",
    "dt_cv = GridSearchCV(estimator=DecisionTreeClassifier(random_state=123),\n",
    "                      param_grid={'criterion': ['gini', 'entropy'],\n",
    "                                  'max_depth': [5, 10, 12, 15, 25, 50]},\n",
    "                      cv=5,\n",
    "                      scoring='recall_weighted',\n",
    "                      return_train_score=True)\n",
    "\n",
    "dt_cv.fit(X_train_clean, y_train)\n",
    "print(f\"Best parameter: {dt_cv.best_params_}\")\n",
    "\n",
    "#train model using best parameter\n",
    "dt_clf = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=123)\n",
    "dt_clf.fit(X_train_clean, y_train)\n",
    "\n",
    "y_pred_dt = dt_clf.predict(X_valid_clean)\n",
    "\n",
    "dt_score = recall_score(y_valid, y_pred_dt, average='weighted')\n",
    "print(dt_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. RandomForest Classifier\n",
    "\n",
    "rf_cv = GridSearchCV(estimator=RandomForestClassifier(random_state=123),\n",
    "                      param_grid={'min_samples_leaf':[2, 10, 30],\n",
    "                                  'n_estimators': [100,200, 350, 500]},\n",
    "                      cv=5,\n",
    "                      scoring='recall_weighted',\n",
    "                      return_train_score=True)\n",
    "\n",
    "rf_cv.fit(X_train_clean, y_train)\n",
    "print(f\"Best parameter: {rf_cv.best_params_}\")\n",
    "\n",
    "#train model using best parameter\n",
    "rf_clf = RandomForestClassifier(min_samples_leaf=2, n_estimators=500, random_state=123)\n",
    "rf_clf.fit(X_train_clean, y_train)\n",
    "\n",
    "y_pred_rf = rf_clf.predict(X_valid_clean)\n",
    "\n",
    "rf_score = recall_score(y_valid, y_pred_rf, average='weighted')\n",
    "print(rf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. AdaBoost Classifier\n",
    "\n",
    "ada_cv = GridSearchCV(estimator=AdaBoostClassifier(random_state=123),\n",
    "                      param_grid={'n_estimators': [50,100,250,400,500,600],\n",
    "                                  'learning_rate': [0.2,0.5,0.8,1]},\n",
    "                      cv=5,\n",
    "                      scoring='recall_weighted',\n",
    "                      return_train_score=True)\n",
    "\n",
    "ada_cv.fit(X_train_clean, y_train)\n",
    "print(f\"Best parameter: {ada_cv.best_params_}\")\n",
    "\n",
    "#train model using best parameter\n",
    "ada_clf = AdaBoostClassifier(n_estimators=500, learning_rate=0.2, random_state=123)\n",
    "ada_clf.fit(X_train_clean, y_train)\n",
    "\n",
    "y_pred_ada = ada_clf.predict(X_valid_clean)\n",
    "\n",
    "ada_score = recall_score(y_valid, y_pred_ada, average='weighted')\n",
    "print(ada_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summarize the model score\n",
    "score_list = {\n",
    "    'Model': ['DummyClassifier', 'LogisticRegression', 'DecisionTree', 'RandomForest', 'AdaBoost'],\n",
    "    'Score': [dummy_score, lr_score, dt_score, rf_score, ada_score]\n",
    "}\n",
    "\n",
    "df_score = pd.DataFrame(score_list)\n",
    "df_score\n",
    "\n",
    "#  Best model is RandomForest => apply to test data\n",
    "\n",
    "y_pred_rf_test = rf_clf.predict(X_test_clean)\n",
    "print(recall_score(y_test, y_pred_rf_test, average='weighted'))\n",
    "print(confusion_matrix(y_test, y_pred_rf_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Summary The model give slighly decrease score on test data, with around 14% of false-negative prediction.\n",
    "\n",
    "Further works should be done to improve sensitivity (recall_score) of the model and decrease the false-negative prediction result. Some recommendation adjustment for further works:\n",
    "\n",
    "Perform class_weight during hyperparameter tuning (CrossValidation), since the dataset is imbalance, the model might be biased towards the majority class, leading to more false negatives.\n",
    "Increase model complexity to reduce underfit, by applying deeper trees (max_depth), increasing the number of trees (n_estimators), and/or Reducing min_samples_leaf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
